{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "spec_file_address = \"https://cse.unl.edu/~pquint/teaching/unl_classes/cse496dl/rl_hackathon/spec-file.txt\"\n",
    "env_name = \"rl_tensorflow\"\n",
    "\n",
    "subprocess.call(\"curl -o spec-file.txt {}\".format(spec_file_address), shell=True)\n",
    "subprocess.call(\"conda create --name {} --file spec-file.txt\".format(env_name), shell=True)\n",
    "subprocess.call(\"python -m ipykernel install --user --name \\\"$CONDA_DEFAULT_ENV\\\" --display-name \\\"Python ($CONDA_DEFAULT_ENV)\\\"\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import atari_wrappers           # from OpenAI Baselines\n",
    "import gym                      # for the RL environments\n",
    "from gym import wrappers\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # for plots\n",
    "%matplotlib inline\n",
    "\n",
    "files = '/work/cse496dl/teams/Dropouts/3_Homework/agent_2/target/'\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /work/cse496dl/teams/Dropouts/3_Homework/agent_2/target/homework_3\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "saver = tf.train.import_meta_graph(files + 'homework_3.meta')\n",
    "saver.restore(session,files + 'homework_3')\n",
    "graph = session.graph\n",
    "x = graph.get_tensor_by_name('input_placeholder:0')\n",
    "output = graph.get_tensor_by_name('output:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = atari_wrappers.wrap_deepmind(atari_wrappers.make_atari('SeaquestNoFrameskip-v4'), clip_rewards=False, frame_stack=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "total_rewards = 0\n",
    "for _ in range(5):\n",
    "    observations = env.reset()\n",
    "    observations = np.array(observations).reshape(1, 84, 84, 4)\n",
    "    while True:\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        Qs = session.run(output, feed_dict = {x: observations})\n",
    "                             \n",
    "        action = np.argmax(Qs)\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            print (\"Score: \", total_rewards)\n",
    "            break\n",
    "\n",
    "        observations =  np.array(next_obs).reshape(1, 84,84, 4)\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl_tensorflow)",
   "language": "python",
   "name": "rl_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
